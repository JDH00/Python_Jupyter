{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68863e83",
   "metadata": {},
   "source": [
    "## 신경망 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6d09c0",
   "metadata": {},
   "source": [
    "## 단순한 신경망 구현: Logic Gate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415cc016",
   "metadata": {},
   "source": [
    "### 필요한 모듈 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f37d2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\coding\\AppData\\Local\\Temp\\ipykernel_17504\\1334896370.py:3: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn-whitegrid')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735860d4",
   "metadata": {},
   "source": [
    "### 하이퍼 파라미터(Hyper Parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3be217c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "lr = 0.1#learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10773dd8",
   "metadata": {},
   "source": [
    "### 유틸 함수들(Utill Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a8c1219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))\n",
    "\n",
    "def mean_squared_error(pred_y,true_y):\n",
    "    return 0.5 * (np.sum(true_y - pred_y)**2)\n",
    "\n",
    "def cross_entropy_error(pred_y,true_y):\n",
    "    if true_y.dim == 1:\n",
    "        true_y = true_y.reshape(1,-1)\n",
    "        pred_y = pred_y.reshape(1,-1)\n",
    "        \n",
    "    delta = 1e-7\n",
    "    return -np.sum(true_y * np.log(pred_y)+delta)\n",
    "\n",
    "def cross_entropy_error_for_batch(pred_y,true_y):\n",
    "    if true_y.dim == 1:\n",
    "        true_y = true_y.reshape(1,-1)\n",
    "        pred_y = pred_y.reshape(1,-1)\n",
    "    \n",
    "    delta = 1e-7\n",
    "    batch_size = pred_y.shape[0]\n",
    "    return -np.sum(true_y * np.log(pred_y)+delta) / batch_size\n",
    "\n",
    "def cross_entropy_error_for_bin(pred_y,true_y):\n",
    "    return 0.5 * np.sum((-true_y*np.log(pred_y)-(1 - true_y) * np.log(1 - pred_y)))\n",
    "\n",
    "def softmax(a):\n",
    "    exp_a = np.exp(a)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    \n",
    "    return y\n",
    "\n",
    "def differential(f,x):\n",
    "    eps = 1e-5\n",
    "    diff_value = np.zeros_like(x)\n",
    "    \n",
    "    for i in range(x.shape[0]):\n",
    "        temp_val = x[i]\n",
    "        \n",
    "        x[i] = temp_val + eps\n",
    "        f_h1 = f(x)\n",
    "        \n",
    "        x[i] = temp_val - eps\n",
    "        f_h2 = f(x)\n",
    "        \n",
    "        diff_value[i] = (f_h1 - f_h2) / (2*eps)\n",
    "        \n",
    "        return diff_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2162b5a9",
   "metadata": {},
   "source": [
    "### 신경망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a157cf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogicGateNet():\n",
    "    \n",
    "    def __init__(self):\n",
    "        def weight_init():\n",
    "            np.random.seed(1)\n",
    "            weights = np.random.randn(2)\n",
    "            bias = np.random.rand(1)\n",
    "            \n",
    "            return weights, bias\n",
    "        \n",
    "        self.weights, self.bias = weight_init()\n",
    "    \n",
    "    def predict(self,x):\n",
    "        W = self.weights.reshape(-1,1)\n",
    "        b = self.bias\n",
    "        \n",
    "        pred_y = sigmoid(np.dot(x,W) + b)\n",
    "        return pred_y\n",
    "    \n",
    "    def loss(self, x, true_y):\n",
    "        pred_y = self.predict(x)\n",
    "        return cross_entropy_error_for_bin(pred_y, true_y)\n",
    "    \n",
    "    def get_gradient(self,x,t):\n",
    "        def loss_grad(grad):\n",
    "            return self.loss(x,t)\n",
    "        grad_W = differential(loss_grad, self.weights)\n",
    "        grad_B = differential(loss_grad, self.bias)\n",
    "        \n",
    "        return grad_W, grad_B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66934fc8",
   "metadata": {},
   "source": [
    "### AND Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd87433a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Cost: 0.9937133417611511, Weights: [ 1.7706656  -0.61175641], Bias: [-1.73184454]\n",
      "Epoch: 200, Cost: 0.9453691534636421, Weights: [ 2.30206775 -0.61175641], Bias: [-2.17412093]\n",
      "Epoch: 300, Cost: 0.9213189601434059, Weights: [ 2.67101724 -0.61175641], Bias: [-2.49489478]\n",
      "Epoch: 400, Cost: 0.9072496410698916, Weights: [ 2.94958142 -0.61175641], Bias: [-2.74503609]\n",
      "Epoch: 500, Cost: 0.898116500593017, Weights: [ 3.17207672 -0.61175641], Bias: [-2.94900524]\n",
      "Epoch: 600, Cost: 0.8917481979725927, Weights: [ 3.3567216  -0.61175641], Bias: [-3.12070187]\n",
      "Epoch: 700, Cost: 0.8870715209537002, Weights: [ 3.51422763 -0.61175641], Bias: [-3.26868498]\n",
      "Epoch: 800, Cost: 0.883499998211386, Weights: [ 3.65138419 -0.61175641], Bias: [-3.39856171]\n",
      "Epoch: 900, Cost: 0.8806878732308581, Weights: [ 3.77274757 -0.61175641], Bias: [-3.51418907]\n",
      "Epoch: 1000, Cost: 0.8784188572990992, Weights: [ 3.88151527 -0.61175641], Bias: [-3.61832613]\n"
     ]
    }
   ],
   "source": [
    "AND = LogicGateNet()\n",
    "\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y = np.array([[0],[0],[0],[1]])\n",
    "\n",
    "train_loss_list = list()\n",
    "\n",
    "for i in range(epochs):\n",
    "    grad_W, grad_B = AND.get_gradient(X,Y)\n",
    "    \n",
    "    AND.weights -= lr*grad_W\n",
    "    AND.bias -= lr*grad_B\n",
    "    \n",
    "    loss = AND.loss(X,Y)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % 100 == 99 :\n",
    "        print(\"Epoch: {}, Cost: {}, Weights: {}, Bias: {}\".format(i+1, loss, AND.weights, AND.bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a723c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coding",
   "language": "python",
   "name": "coding"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
